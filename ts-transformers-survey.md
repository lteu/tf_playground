# Paper survey
## IJCAI 2023
- Yiduo Li, Shiyi Qi, Zhe Li, Zhongwen Rao, Lujia Pan, Zenglin Xu:
SMARTformer: Semi-Autoregressive Transformer with Efficient Integrated Window Attention for Long Time Series Forecasting. 2169-2177

- Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, Liang Sun:
Transformers in Time Series: A Survey.


## AAAI 2023
- Ailing Zeng, Muxi Chen, Lei Zhang, Qiang Xu:
Are Transformers Effective for Time Series Forecasting?

## AAAI 2022
-	Linyi Yang, Jiazheng Li, Ruihai Dong, Yue Zhang, Barry Smyth:
NumHTML: Numeric-Oriented Hierarchical Transformer Model for Multi-Task Financial Forecasting. 11604-11612

## ICML 2022
- 	Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, Rong Jin:
FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting. 27268-27286

- Weizhe Hua, Zihang Dai, Hanxiao Liu, Quoc V. Le:
Transformer Quality in Linear Time. 9099-9117


## NIPS 2021

- 	Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long:
Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting. 22419-22430

- Kamile Stankeviciute, Ahmed M. Alaa, Mihaela van der Schaar:
Conformal Time-series Forecasting. 6216-6228

- Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, Yunhe Wang:
Transformer in Transformer. 15908-15919

- 	Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao, Chunjing Xu, Tao Xiang, Li Zhang:
SOFT: Softmax-free Transformer with Linear Complexity. 21297-21309

- Binh Tang, David S. Matteson:
Probabilistic Transformer For Time Series Analysis. 23592-23608


## ICML 2021
- Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, Che Zheng:
Synthesizer: Rethinking Self-Attention for Transformer Models. 10183-10192

- Gail Weiss, Yoav Goldberg, Eran Yahav:
Thinking Like Transformers. 11080-11090

- 	Noam Wies, Yoav Levine, Daniel Jannai, Amnon Shashua:
Which transformer architecture fits my data? A vocabulary bottleneck in self-attention. 11170-11181


